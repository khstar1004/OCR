import os
import re
import time
import logging
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import concurrent.futures
import threading

from ..utils.config import config
from ..utils.logger import get_logger
from ..utils.memory_manager import MemoryManager

# ë¡œê±° ì„¤ì •
logger = get_logger()

class KSMROCrawler:
    """KSMRO ì›¹ì‚¬ì´íŠ¸ ì „ìš© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_workers=None, base_url=None):
        self.base_url = base_url or config.crawler.base_url
        self.max_workers = max_workers or config.processing.max_workers
        self.memory_manager = MemoryManager(config.processing.memory_limit)
        
        # ì„¸ì…˜ ì„¤ì • ê°œì„ 
        self.session = requests.Session()
        self.session.headers.update(config.crawler.headers)
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=self.max_workers,
            pool_maxsize=self.max_workers * 2,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        self.found_product_ids = set()
        self.visited_urls = set()
        self.chrome_options = self._setup_chrome_options()
        self.driver = None
        self.driver_lock = threading.RLock()  # ë“œë¼ì´ë²„ ì ‘ê·¼ì„ ìœ„í•œ ë½ ì¶”ê°€
        
    def _setup_chrome_options(self):
        """Chrome ì›¹ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-infobars")
        return chrome_options
    
    def _init_selenium(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” (í•„ìš”í•  ë•Œë§Œ)"""
        with self.driver_lock:
            if self.driver is None:
                try:
                    self.driver = webdriver.Chrome(options=self.chrome_options)
                    logger.info("Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    raise
    
    def _close_selenium(self):
        """Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        with self.driver_lock:
            if self.driver:
                try:
                    self.driver.quit()
                    logger.debug("Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ì‹¤íŒ¨: {str(e)}")
                finally:
                    self.driver = None
    
    def get_product_details(self, product_url):
        """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€)"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            response = requests.get(product_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë©”íƒ€ íƒœê·¸ì—ì„œ ì•ˆì „í•˜ê²Œ ì •ë³´ ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •)
            product_name_tag = soup.find("meta", property="og:title")
            product_image_tag = soup.find("meta", property="og:image")
            product_description_tag = soup.find("meta", property="og:description")

            product_name = product_name_tag["content"].strip() if product_name_tag else ""
            product_image = product_image_tag["content"].strip() if product_image_tag else ""
            product_description = product_description_tag["content"].strip() if product_description_tag else ""

            # ìƒí’ˆ ê°€ê²© ì¶”ì¶œ (í˜ì´ì§€ ë‚´ íŠ¹ì • ìš”ì†Œì—ì„œ ì¶”ì¶œ, ì˜ˆì‹œ)
            price_element = soup.select_one(".price_box .price strong")
            product_price = price_element.get_text(strip=True) if price_element else ""

            # ìƒí’ˆ ìŠ¤í™ ì¶”ì¶œ (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
            specs = {}
            spec_rows = soup.select(".vi_txt_li dl")
            for row in spec_rows:
                dt = row.select_one("dt")
                dd = row.select_one("dd")
                if dt and dd:
                    specs[dt.get_text(strip=True)] = dd.get_text(strip=True)

            # ì´ë¯¸ì§€ ëª©ë¡ ì¶”ì¶œ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
            images = [product_image] if product_image else []

            # ìƒí’ˆ ì •ë³´ ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
            if not product_name or product_name == "ìƒí’ˆëª… ì •ë³´ ì—†ìŒ":
                logger.warning(f"ìƒí’ˆ ì •ë³´ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤: {product_url}")
                return None

            product_details = {
                "url": product_url,
                "name": product_name,
                "image": product_image,
                "description": product_description,
                "price": product_price,
                "specs": specs,
                "images": images
            }

            logger.info(f"ìƒí’ˆ í¬ë¡¤ë§ ì„±ê³µ: {product_url}")
            return product_details

        except requests.exceptions.RequestException as e:
            logger.error(f"HTTP ìš”ì²­ ì‹¤íŒ¨: {product_url} - {str(e)}")
        except Exception as e:
            logger.error(f"ìƒí’ˆ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {product_url} - {str(e)}")

        return None
    
    def _extract_title(self, soup):
        """ìƒí’ˆ ì œëª© ì¶”ì¶œ"""
        title_elem = soup.select_one('h3.item_detail')
        if title_elem:
            return title_elem.text.strip()
        
        # ëŒ€ì²´ ë°©ë²•
        title_elem = soup.select_one('.sit_title')
        if title_elem:
            return title_elem.text.strip()
            
        return "ì œëª© ì—†ìŒ"
    
    def _extract_price(self, soup):
        """ìƒí’ˆ ê°€ê²© ì¶”ì¶œ"""
        price_elem = soup.select_one('.price .mpr')
        if price_elem:
            price_text = price_elem.text.strip()
            # ìˆ«ìë§Œ ì¶”ì¶œ
            price = re.sub(r'[^\d]', '', price_text)
            return price
        return "ê°€ê²© ì •ë³´ ì—†ìŒ"
    
    def _extract_specs(self, soup):
        """ìƒí’ˆ ìŠ¤í™ ì •ë³´ ì¶”ì¶œ"""
        specs = {}
        
        # ìƒí’ˆì •ë³´ì œê³µê³ ì‹œ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
        spec_table = soup.select_one('.tbl_frm01 table')
        if spec_table:
            rows = spec_table.select('tr')
            for row in rows:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td:
                    key = th.text.strip()
                    value = td.text.strip()
                    specs[key] = value
        
        # ìƒí’ˆ ì˜µì…˜ ì •ë³´
        option_elems = soup.select('.sit_option th, .sit_option td')
        for i in range(0, len(option_elems), 2):
            if i+1 < len(option_elems):
                key = option_elems[i].text.strip()
                value = option_elems[i+1].text.strip()
                specs[key] = value
                
        return specs
    
    def _extract_description(self, soup):
        """ìƒí’ˆ ìƒì„¸ ì„¤ëª… ì¶”ì¶œ"""
        desc_elem = soup.select_one('.ofh.tac.padt10.padb10')
        if desc_elem:
            return desc_elem.text.strip()
        
        # ëŒ€ì²´ ë°©ë²•
        desc_elem = soup.select_one('#sit_inf_explan')
        if desc_elem:
            return desc_elem.text.strip()
            
        return "ìƒì„¸ ì„¤ëª… ì—†ìŒ"
    
    def _extract_images(self, soup, base_url):
        """ìƒí’ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        image_urls = []
        
        # ë©”ì¸ ì´ë¯¸ì§€
        main_img = soup.select_one('#sit_pvi_big img')
        if main_img and main_img.get('src'):
            img_url = urljoin(base_url, main_img['src'])
            image_urls.append(img_url)
        
        # ì¶”ê°€ ì´ë¯¸ì§€ë“¤
        extra_imgs = soup.select('.sit_pvi_thumb img')
        for img in extra_imgs:
            if img.get('src'):
                img_url = urljoin(base_url, img['src'])
                image_urls.append(img_url)
        
        # ìƒì„¸ ì„¤ëª… ë‚´ ì´ë¯¸ì§€
        desc_imgs = soup.select('.ofh.tac.padt10.padb10 img')
        for img in desc_imgs:
            if img.get('src'):
                img_url = urljoin(base_url, img['src'])
                image_urls.append(img_url)
                
        return image_urls
    
    def _extract_category(self, soup):
        """ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        try:
            # ì¹´í…Œê³ ë¦¬ ê²½ë¡œ ì¶”ì¶œ
            category_elem = soup.select_one('.sct_here')
            if category_elem:
                return category_elem.text.strip()
            
            # ëŒ€ì²´ ë°©ë²•
            breadcrumb = soup.select('.sct_here a')
            if breadcrumb:
                return ' > '.join([a.text.strip() for a in breadcrumb])
            
        except Exception as e:
            logger.debug(f"ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return "ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ"
    
    def crawl_category(self, category_url, max_pages=5):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í¬ë¡¤ë§"""
        product_urls = []
        
        try:
            logger.info(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘: {category_url}")
            self._init_selenium()
            
            for page in range(1, max_pages + 1):
                page_url = f"{category_url}&page={page}" if '?' in category_url else f"{category_url}?page={page}"
                
                retry_count = 0
                max_retries = 3
                while retry_count < max_retries:
                    try:
                        self.driver.get(page_url)
                        WebDriverWait(self.driver, 20).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, ".pr_desc a, .sct_txt a"))
                        )
                        
                        # ìƒí’ˆ ë§í¬ ì¶”ì¶œ
                        product_links = self.driver.find_elements(By.CSS_SELECTOR, ".pr_desc a[href*='view.php?index_no='], .sct_txt a[href*='view.php?index_no=']")
                        
                        if not product_links:
                            logger.info(f"ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€: {page}")
                            break
                        
                        for link in product_links:
                            product_url = link.get_attribute('href')
                            if product_url and product_url not in self.visited_urls:
                                product_urls.append(product_url)
                                self.visited_urls.add(product_url)
                        
                        logger.info(f"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ, {len(product_links)}ê°œ ìƒí’ˆ ë°œê²¬")
                        
                        # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ ì¶”ê°€
                        time.sleep(2)
                        
                        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ retry ë£¨í”„ íƒˆì¶œ
                        break
                        
                    except TimeoutException:
                        retry_count += 1
                        logger.warning(f"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ({retry_count}/{max_retries}): {page_url}")
                        time.sleep(3)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                        if retry_count >= max_retries:
                            logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ë¡œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {page_url}")
                            break
                    except Exception as e:
                        retry_count += 1
                        logger.error(f"í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ({retry_count}/{max_retries}): {page_url} - {str(e)}")
                        time.sleep(3)
                        if retry_count >= max_retries:
                            logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ë¡œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {page_url}")
                            break
        
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return product_urls
    
    def search_products(self, keyword, max_pages=5):
        """í‚¤ì›Œë“œë¡œ ìƒí’ˆ ê²€ìƒ‰"""
        search_url = f"{self.base_url}/shop/search.php?ss_tx={keyword}"
        return self.crawl_category(search_url, max_pages)
    
    def crawl_product_ids(self, id_range):
        """ID ë²”ìœ„ë¡œ ìƒí’ˆ í¬ë¡¤ë§"""
        product_urls = []
        start_id, end_id = map(int, id_range.split('-'))
        
        logger.info(f"ID ë²”ìœ„ {start_id}-{end_id}ë¡œ ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for product_id in range(start_id, end_id + 1):
                product_url = f"{self.base_url}/shop/view.php?index_no={product_id}"
                futures.append(executor.submit(self._check_product_exists, product_url, product_id))
            
            for future in futures:
                result = future.result()
                if result:
                    product_urls.append(result)
        
        logger.info(f"ID ë²”ìœ„ í¬ë¡¤ë§ ì™„ë£Œ, {len(product_urls)}ê°œ ìœ íš¨í•œ ìƒí’ˆ ë°œê²¬")
        return product_urls
    
    def _check_product_exists(self, url, product_id):
        """ìƒí’ˆ URLì´ ìœ íš¨í•œì§€ í™•ì¸"""
        try:
            response = self.session.get(url, timeout=config.crawler.timeout)
            if response.status_code == 200 and "item_detail" in response.text:
                self.found_product_ids.add(str(product_id))
                logger.debug(f"ìœ íš¨í•œ ìƒí’ˆ ID ë°œê²¬: {product_id}")
                return url
        except Exception as e:
            logger.debug(f"ìƒí’ˆ ID {product_id} í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None
    
    def download_image(self, image_url, save_path):
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(image_url, stream=True, timeout=config.crawler.timeout)
            if response.status_code == 200:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
                return True
            else:
                logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False
    
    def export_product_ids(self, output_path="found_product_ids.txt"):
        """ë°œê²¬ëœ ìƒí’ˆ ID ë‚´ë³´ë‚´ê¸°"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(list(self.found_product_ids), f, ensure_ascii=False)
            logger.info(f"{len(self.found_product_ids)}ê°œì˜ ìƒí’ˆ IDë¥¼ {output_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ìƒí’ˆ ID ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self._close_selenium()
        if self.session:
            self.session.close()

    def generate_product_urls(self, id_patterns=None, start_id=None, end_id=None, scan_all_categories=False):
        """
        ìƒí’ˆ URL ìƒì„±
        
        Args:
            id_patterns: ID íŒ¨í„´ ëª©ë¡ (ì˜ˆ: ['102XXXXX', '103XXXXX'])
            start_id: ID ë²”ìœ„ ì‹œì‘
            end_id: ID ë²”ìœ„ ë
            scan_all_categories: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì—¬ë¶€
            
        Returns:
            ìƒì„±ëœ ìƒí’ˆ URL ëª©ë¡
        """
        product_urls = []
        
        # 1. ID íŒ¨í„´ ê¸°ë°˜ URL ìƒì„±
        if id_patterns:
            logger.info(f"ID íŒ¨í„´ ê¸°ë°˜ URL ìƒì„± ì¤‘...")
            for pattern in id_patterns:
                if 'X' in pattern:
                    # Xê°€ ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸°
                    x_positions = [i for i, char in enumerate(pattern) if char == 'X']
                    
                    # X ê°œìˆ˜ì— ë”°ë¼ ì „ëµ ì¡°ì •
                    if len(x_positions) > 4:  # Xê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ëœë¤ ìƒ˜í”Œë§
                        import random
                        for _ in range(1000):  # ìµœëŒ€ 1000ê°œ ID ìƒì„±
                            id_copy = list(pattern)
                            for pos in x_positions:
                                id_copy[pos] = random.choice('0123456789')
                            product_id = ''.join(id_copy)
                            product_url = f"{self.base_url}/shop/view.php?index_no={product_id}"
                            product_urls.append(product_url)
                    else:
                        # ëª¨ë“  ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
                        import itertools
                        for digits in itertools.product('0123456789', repeat=len(x_positions)):
                            id_copy = list(pattern)
                            for i, pos in enumerate(x_positions):
                                id_copy[pos] = digits[i]
                            product_id = ''.join(id_copy)
                            product_url = f"{self.base_url}/shop/view.php?index_no={product_id}"
                            product_urls.append(product_url)
                else:
                    # Xê°€ ì—†ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    product_url = f"{self.base_url}/shop/view.php?index_no={pattern}"
                    product_urls.append(product_url)
        
        # 2. ID ë²”ìœ„ ê¸°ë°˜ URL ìƒì„±
        elif start_id and end_id:
            logger.info(f"ID ë²”ìœ„ {start_id}-{end_id} ê¸°ë°˜ URL ìƒì„± ì¤‘...")
            try:
                start = int(start_id)
                end = int(end_id)
                for product_id in range(start, end + 1):
                    product_url = f"{self.base_url}/shop/view.php?index_no={product_id}"
                    product_urls.append(product_url)
            except ValueError:
                logger.error("ID ë²”ìœ„ëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        # 3. ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìŠ¤ìº”
        elif scan_all_categories:
            logger.info("ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìŠ¤ìº” ì¤‘...")
            try:
                # ëŒ€ì‹  ID ë²”ìœ„ë¡œ URL ìƒì„± (ì˜ˆì‹œ ID ë²”ìœ„: 1-200000)
                start_id = 1
                end_id = 200000  # ì ì ˆí•œ ìµœëŒ€ ID ë²”ìœ„ ì„¤ì •
                logger.info(f"ID ë²”ìœ„ {start_id}-{end_id}ë¡œ URL ìƒì„±")
                
                for product_id in range(start_id, end_id + 1):
                    product_url = f"{self.base_url}/shop/view.php?index_no={product_id}"
                    product_urls.append(product_url)
                
            except Exception as e:
                logger.error(f"URL ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        logger.info(f"ì´ {len(product_urls)}ê°œ ìƒí’ˆ URL ìƒì„± ì™„ë£Œ")
        return product_urls
    
    def crawl_products(self, product_urls):
        """
        ì—¬ëŸ¬ ìƒí’ˆ URLì„ í¬ë¡¤ë§í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            product_urls: í¬ë¡¤ë§í•  ìƒí’ˆ URL ëª©ë¡
            
        Returns:
            ìˆ˜ì§‘ëœ ì›¹ ë°ì´í„° (URLì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬)
        """
        web_data = {}
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ í’€ ìƒì„± - ìµœëŒ€ ì‘ì—…ì ìˆ˜ ì œí•œ
        max_workers = min(self.max_workers, 5)  # ìµœëŒ€ 5ê°œ ìŠ¤ë ˆë“œë¡œ ì œí•œ
        logger.info(f"ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘: {len(product_urls)}ê°œ URL, {max_workers}ê°œ ìŠ¤ë ˆë“œ ì‚¬ìš©")
        
        # ì§„í–‰ ìƒí™© ì¶”ì  ë³€ìˆ˜
        total_urls = len(product_urls)
        processed_urls = 0
        successful_urls = 0
        failed_urls = 0
        skipped_urls = 0  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒí’ˆ
        
        print(f"\n{'='*50}")
        print(f"ğŸ” ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘: ì´ {total_urls}ê°œ URL")
        print(f"{'='*50}")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
        progress_bar = tqdm(total=total_urls, desc="ìƒí’ˆ í¬ë¡¤ë§", unit="í˜ì´ì§€")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # URLë³„ë¡œ ì‘ì—… ì œì¶œ
            future_to_url = {executor.submit(self.crawl_product, url): url for url in product_urls}
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                processed_urls += 1
                
                try:
                    data = future.result()
                    if data:
                        web_data[url] = data
                        successful_urls += 1
                        progress_bar.set_description(f"âœ… ì„±ê³µ: {successful_urls}/{processed_urls}")
                    else:
                        # ìƒí’ˆì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                        skipped_urls += 1
                        progress_bar.set_description(f"â© ê±´ë„ˆëœ€: {skipped_urls}/{processed_urls}")
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬
                    if processed_urls % 10 == 0:
                        self.memory_manager.check_memory()
                        
                except Exception as e:
                    failed_urls += 1
                    logger.error(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}")
                    progress_bar.set_description(f"âŒ ì‹¤íŒ¨: {failed_urls}/{processed_urls}")
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                progress_bar.update(1)
                
                # ìƒì„¸ ìƒíƒœ ì •ë³´ ì—…ë°ì´íŠ¸ (5% ë‹¨ìœ„ë¡œ)
                if processed_urls % max(1, total_urls // 20) == 0 or processed_urls == total_urls:
                    progress_percent = (processed_urls / total_urls) * 100
                    progress_bar.set_postfix({
                        "ì§„í–‰ë¥ ": f"{progress_percent:.1f}%", 
                        "ì„±ê³µ": successful_urls,
                        "ì‹¤íŒ¨": failed_urls, 
                        "ê±´ë„ˆëœ€": skipped_urls
                    })
                    
                    # ì£¼ê¸°ì ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ì¶œë ¥
                    if processed_urls % max(1, total_urls // 5) == 0:
                        print(f"\nğŸ“Š ì§„í–‰ ìƒí™© (ì²˜ë¦¬: {processed_urls}/{total_urls}):")
                        print(f"  - ì„±ê³µ: {successful_urls}ê°œ ({(successful_urls/processed_urls)*100:.1f}%)")
                        print(f"  - ì‹¤íŒ¨: {failed_urls}ê°œ ({(failed_urls/processed_urls)*100:.1f}%)")
                        print(f"  - ê±´ë„ˆëœ€: {skipped_urls}ê°œ ({(skipped_urls/processed_urls)*100:.1f}%)")
        
        # ì§„í–‰ ë°” ë‹«ê¸°
        progress_bar.close()
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼:")
        print(f"  - ì´ URL: {total_urls}ê°œ")
        print(f"  - ì„±ê³µ: {successful_urls}ê°œ ({(successful_urls/total_urls)*100:.1f}%)")
        print(f"  - ì‹¤íŒ¨: {failed_urls}ê°œ ({(failed_urls/total_urls)*100:.1f}%)")
        print(f"  - ì—†ëŠ” ìƒí’ˆ: {skipped_urls}ê°œ ({(skipped_urls/total_urls)*100:.1f}%)")
        print(f"{'='*50}")
        
        logger.info(f"ì´ {len(web_data)}ê°œ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
        return web_data

    def crawl_product(self, url):
        """
        ë‹¨ì¼ ìƒí’ˆ URL í¬ë¡¤ë§
        
        Args:
            url: í¬ë¡¤ë§í•  ìƒí’ˆ URL
            
        Returns:
            ìˆ˜ì§‘ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ìƒí’ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            product_info = self.get_product_details(url)
            if not product_info:
                return None
            
            # í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ URL ì¶”ì¶œ
            text = f"{product_info['name']} {product_info['description']} {' '.join(str(spec) for spec in product_info['specs'].values())}"
            images = product_info['images']
            
            return {
                'text': text,
                'images': images,
                'product_info': product_info
            }
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {url} - {str(e)}")
            return None

    def export_found_product_ids(self, output_path):
        """
        ë°œê²¬ëœ ìƒí’ˆ IDë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
        
        Args:
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ID ëª©ë¡ ì €ì¥
            with open(output_path, 'w', encoding='utf-8') as f:
                for product_id in sorted(self.found_product_ids):
                    f.write(f"{product_id}\n")
                
            logger.info(f"{len(self.found_product_ids)}ê°œ ìƒí’ˆ IDë¥¼ {output_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ID ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}") 